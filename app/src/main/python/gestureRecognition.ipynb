{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references: https://github.com/nicknochnack/ActionDetectionforSignLanguage.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR to RGB\n",
    "    image.flags.writeable = False                  # Image is not writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB to BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks,   mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thank you', 'book'])\n",
    "\n",
    "# 20 videos of data\n",
    "num_sequences = 20\n",
    "\n",
    "# Total sequences after augmentation (original + flipped + noisy)\n",
    "num_augmented_sequences = num_sequences * 2  \n",
    "total_sequences = num_sequences + num_augmented_sequences  \n",
    "\n",
    "# Videos of 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created successfully.\n"
     ]
    }
   ],
   "source": [
    "for action in actions: \n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    os.makedirs(action_path, exist_ok=True)\n",
    "\n",
    "    # Create subdirectories for each type of transformation\n",
    "    for subfolder in ['original', 'flipped', 'noisy']:\n",
    "        os.makedirs(os.path.join(action_path, subfolder), exist_ok=True)\n",
    "        \n",
    "      # Get existing sequence folders and find the highest folder number\n",
    "    files = os.listdir(action_path)\n",
    "    dirmax = max([int(f) for f in files if f.isdigit()], default=0)\n",
    "    \n",
    "    # Loop to create new sequence folders (for both original and augmented sequences)\n",
    "    for sequence in range(start_folder, total_sequences + 1):\n",
    "        sequence_folder = str(dirmax + sequence)\n",
    "        sequence_folder_path = os.path.join(action_path, sequence_folder)\n",
    "        \n",
    "        # Create sequence folder and subdirectories\n",
    "        os.makedirs(sequence_folder_path, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories under each sequence folder (if not already created)\n",
    "        for subfolder in ['original', 'flipped', 'noisy']:\n",
    "            os.makedirs(os.path.join(sequence_folder_path, subfolder), exist_ok=True)\n",
    "\n",
    "print(\"Directories created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VID_PATH = os.path.join('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection and augmentation complete.\n"
     ]
    }
   ],
   "source": [
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, start_folder+num_sequences):\n",
    "            video_file = os.path.join(VID_PATH, action, f\"{sequence}.mp4\")\n",
    "            if not os.path.exists(video_file):\n",
    "                print(f\"Skipping {video_file}, file not found.\")\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            \n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Warning: Missing frame {frame_num} in {video_file}\")\n",
    "                    break\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # Display collection status\n",
    "                text = f\"Collecting frames for {action}, Video {sequence}\"\n",
    "                cv2.putText(image, text, (15,12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 225), 1, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # Original keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), 'original', f\"{frame_num}.npy\")\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Data Augmentation: Horizontal Flip\n",
    "                flipped_keypoints = keypoints.copy()\n",
    "                flipped_keypoints[::3] *= -1  # Flip x-coordinates\n",
    "                flipped_npy_path = os.path.join(DATA_PATH, action, str(sequence), 'flipped', f\"{frame_num}.npy\")\n",
    "                np.save(flipped_npy_path, flipped_keypoints)\n",
    "\n",
    "                # Data Augmentation: Add Noise\n",
    "                noise = np.random.normal(0, 0.01, keypoints.shape)\n",
    "                noisy_keypoints = keypoints + noise\n",
    "                noisy_npy_path = os.path.join(DATA_PATH, action, str(sequence), 'noisy', f\"{frame_num}.npy\")\n",
    "                np.save(noisy_npy_path, noisy_keypoints)\n",
    "\n",
    "                # Break using q\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "            cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "print(\"Data collection and augmentation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thank you': 1, 'book': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(1, num_sequences+1):\n",
    "        # Initialize the list for storing frames in the current sequence\n",
    "        original_window = []\n",
    "        flipped_window = []\n",
    "        noisy_window = []\n",
    "        \n",
    "        for frame_num in range(sequence_length):\n",
    "            # Load the original keypoints for the current frame\n",
    "            original_res = np.load(os.path.join(DATA_PATH, action, str(sequence), 'original', \"{}.npy\".format(frame_num)))\n",
    "            original_window.append(original_res)\n",
    "\n",
    "            # Load the flipped keypoints for the current frame\n",
    "            flipped_res = np.load(os.path.join(DATA_PATH, action, str(sequence), 'flipped', \"{}.npy\".format(frame_num)))\n",
    "            flipped_window.append(flipped_res)\n",
    "\n",
    "            # Load the noisy keypoints for the current frame\n",
    "            noisy_res = np.load(os.path.join(DATA_PATH, action, str(sequence), 'noisy', \"{}.npy\".format(frame_num)))\n",
    "            noisy_window.append(noisy_res)\n",
    "\n",
    "        # Append the original, flipped, and noisy windows to the sequences list\n",
    "        sequences.append(original_window)  # Original sequence\n",
    "        sequences.append(flipped_window)   # Flipped sequence\n",
    "        sequences.append(noisy_window)     # Noisy sequence\n",
    "\n",
    "        label = label_map[action]\n",
    "        labels.append(label)  # Original sequence label\n",
    "        labels.append(label)  # Flipped sequence label\n",
    "        labels.append(label)  # Noisy sequence label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 30, 1662)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 30, 1662)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "Input(shape=(30,1662)),\n",
    "LSTM(64, return_sequences=True, activation='tanh'),\n",
    "Dropout(0.2),  # Dropout to prevent overfitting, for regularization\n",
    "LSTM(128, return_sequences=True, activation='tanh'),\n",
    "Dropout(0.2),\n",
    "LSTM(64, return_sequences=False, activation='tanh'),\n",
    "Dropout(0.2),\n",
    "Dense(64, activation='relu'),\n",
    "BatchNormalization(),  # BatchNormalization to stabilize learning\n",
    "Dropout(0.2),  # Dropout for regularization again\n",
    "Dense(32, activation='relu'),\n",
    "BatchNormalization(),\n",
    "Dropout(0.2), \n",
    "Dense(actions.shape[0], activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 0.4068 - loss: 1.2207 - val_accuracy: 0.3333 - val_loss: 1.0717\n",
      "Epoch 2/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.4173 - loss: 1.1015 - val_accuracy: 0.3333 - val_loss: 1.0790\n",
      "Epoch 3/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.3574 - loss: 1.1986 - val_accuracy: 0.4444 - val_loss: 1.0925\n",
      "Epoch 4/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.3493 - loss: 1.2602 - val_accuracy: 0.4444 - val_loss: 1.0859\n",
      "Epoch 5/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.3812 - loss: 1.2300 - val_accuracy: 0.3333 - val_loss: 1.0780\n",
      "Epoch 6/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.3155 - loss: 1.2543 - val_accuracy: 0.3333 - val_loss: 1.0833\n",
      "Epoch 7/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.4687 - loss: 1.0555 - val_accuracy: 0.4444 - val_loss: 1.0742\n",
      "Epoch 8/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.4458 - loss: 1.1608 - val_accuracy: 0.3333 - val_loss: 1.0921\n",
      "Epoch 9/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.3883 - loss: 1.1671 - val_accuracy: 0.3333 - val_loss: 1.1043\n",
      "Epoch 10/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.3694 - loss: 1.2226 - val_accuracy: 0.3333 - val_loss: 1.1339\n",
      "Epoch 11/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.4142 - loss: 1.1257 - val_accuracy: 0.3333 - val_loss: 1.1205\n",
      "Epoch 12/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.3997 - loss: 1.2191 - val_accuracy: 0.3333 - val_loss: 1.0955\n",
      "Epoch 13/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.3933 - loss: 1.1762 - val_accuracy: 0.3333 - val_loss: 1.0816\n",
      "Epoch 14/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.4274 - loss: 1.1875 - val_accuracy: 0.4444 - val_loss: 1.0618\n",
      "Epoch 15/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.4232 - loss: 1.1054 - val_accuracy: 0.5556 - val_loss: 1.0577\n",
      "Epoch 16/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.3817 - loss: 1.1857 - val_accuracy: 0.2222 - val_loss: 1.0736\n",
      "Epoch 17/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.4294 - loss: 1.0566 - val_accuracy: 0.4444 - val_loss: 1.0602\n",
      "Epoch 18/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.5453 - loss: 0.9649 - val_accuracy: 0.4444 - val_loss: 1.1238\n",
      "Epoch 19/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.4492 - loss: 1.1652 - val_accuracy: 0.5556 - val_loss: 1.0271\n",
      "Epoch 20/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.3933 - loss: 1.1136 - val_accuracy: 0.4444 - val_loss: 1.0471\n",
      "Epoch 21/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.5471 - loss: 0.9537 - val_accuracy: 0.4444 - val_loss: 1.0588\n",
      "Epoch 22/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.5942 - loss: 0.8279 - val_accuracy: 0.5556 - val_loss: 0.9983\n",
      "Epoch 23/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.5851 - loss: 0.9383 - val_accuracy: 0.5556 - val_loss: 1.0100\n",
      "Epoch 24/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.6190 - loss: 0.8324 - val_accuracy: 0.4444 - val_loss: 0.9347\n",
      "Epoch 25/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.7142 - loss: 0.7613 - val_accuracy: 0.6667 - val_loss: 0.8615\n",
      "Epoch 26/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.5706 - loss: 0.8723 - val_accuracy: 0.3333 - val_loss: 1.3588\n",
      "Epoch 27/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.6160 - loss: 0.9323 - val_accuracy: 0.3333 - val_loss: 1.3630\n",
      "Epoch 28/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.6455 - loss: 0.8767 - val_accuracy: 0.4444 - val_loss: 1.1407\n",
      "Epoch 29/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.6108 - loss: 0.8217 - val_accuracy: 0.5556 - val_loss: 0.9882\n",
      "Epoch 30/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.6613 - loss: 0.7979 - val_accuracy: 0.5556 - val_loss: 0.8397\n",
      "Epoch 31/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.6044 - loss: 0.8495 - val_accuracy: 0.4444 - val_loss: 1.2655\n",
      "Epoch 32/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.6867 - loss: 0.7838 - val_accuracy: 0.6667 - val_loss: 0.7560\n",
      "Epoch 33/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7458 - loss: 0.6644 - val_accuracy: 0.8889 - val_loss: 0.6478\n",
      "Epoch 34/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.8102 - loss: 0.5906 - val_accuracy: 0.4444 - val_loss: 1.0860\n",
      "Epoch 35/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7498 - loss: 0.5739 - val_accuracy: 0.6667 - val_loss: 0.7568\n",
      "Epoch 36/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.8001 - loss: 0.5399 - val_accuracy: 0.6667 - val_loss: 0.6439\n",
      "Epoch 37/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.8420 - loss: 0.4974 - val_accuracy: 0.7778 - val_loss: 0.5283\n",
      "Epoch 38/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8055 - loss: 0.4661 - val_accuracy: 0.6667 - val_loss: 0.7555\n",
      "Epoch 39/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.8825 - loss: 0.3683 - val_accuracy: 0.7778 - val_loss: 0.6078\n",
      "Epoch 40/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.8218 - loss: 0.4315 - val_accuracy: 0.5556 - val_loss: 0.7050\n",
      "Epoch 41/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.8369 - loss: 0.5452 - val_accuracy: 0.6667 - val_loss: 0.7579\n",
      "Epoch 42/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.8209 - loss: 0.4949 - val_accuracy: 0.7778 - val_loss: 0.6403\n",
      "Epoch 43/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.7973 - loss: 0.5161 - val_accuracy: 0.4444 - val_loss: 1.0374\n",
      "Epoch 44/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.7526 - loss: 0.5218 - val_accuracy: 0.7778 - val_loss: 0.5673\n",
      "Epoch 45/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.8605 - loss: 0.4006 - val_accuracy: 0.7778 - val_loss: 0.6816\n",
      "Epoch 46/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.8400 - loss: 0.3985 - val_accuracy: 0.6667 - val_loss: 0.7182\n",
      "Epoch 47/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.8641 - loss: 0.3718 - val_accuracy: 0.8889 - val_loss: 0.3639\n",
      "Epoch 48/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.9109 - loss: 0.2897 - val_accuracy: 0.8889 - val_loss: 0.4184\n",
      "Epoch 49/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8630 - loss: 0.3132 - val_accuracy: 0.8889 - val_loss: 0.3683\n",
      "Epoch 50/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9151 - loss: 0.2462 - val_accuracy: 1.0000 - val_loss: 0.3112\n",
      "Epoch 51/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9229 - loss: 0.2774 - val_accuracy: 0.7778 - val_loss: 0.3425\n",
      "Epoch 52/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9076 - loss: 0.2212 - val_accuracy: 0.8889 - val_loss: 0.4236\n",
      "Epoch 53/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.8750 - loss: 0.3364 - val_accuracy: 0.8889 - val_loss: 0.5208\n",
      "Epoch 54/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9319 - loss: 0.2449 - val_accuracy: 1.0000 - val_loss: 0.1508\n",
      "Epoch 55/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9042 - loss: 0.2976 - val_accuracy: 0.7778 - val_loss: 0.5871\n",
      "Epoch 56/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.8828 - loss: 0.3799 - val_accuracy: 0.8889 - val_loss: 0.1502\n",
      "Epoch 57/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9221 - loss: 0.2514 - val_accuracy: 0.6667 - val_loss: 0.5368\n",
      "Epoch 58/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9144 - loss: 0.2737 - val_accuracy: 0.5556 - val_loss: 1.8878\n",
      "Epoch 59/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.7130 - loss: 0.8361 - val_accuracy: 0.6667 - val_loss: 1.6010\n",
      "Epoch 60/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8253 - loss: 0.4083 - val_accuracy: 0.5556 - val_loss: 1.2867\n",
      "Epoch 61/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.8447 - loss: 0.4456 - val_accuracy: 0.5556 - val_loss: 1.4750\n",
      "Epoch 62/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.8762 - loss: 0.3524 - val_accuracy: 0.7778 - val_loss: 0.6341\n",
      "Epoch 63/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8114 - loss: 0.3904 - val_accuracy: 0.7778 - val_loss: 0.4525\n",
      "Epoch 64/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.8809 - loss: 0.3177 - val_accuracy: 0.7778 - val_loss: 0.4513\n",
      "Epoch 65/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8904 - loss: 0.3212 - val_accuracy: 0.7778 - val_loss: 0.6093\n",
      "Epoch 66/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9075 - loss: 0.2904 - val_accuracy: 1.0000 - val_loss: 0.2294\n",
      "Epoch 67/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9080 - loss: 0.2379 - val_accuracy: 1.0000 - val_loss: 0.1738\n",
      "Epoch 68/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9534 - loss: 0.2052 - val_accuracy: 0.8889 - val_loss: 0.2418\n",
      "Epoch 69/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9593 - loss: 0.1878 - val_accuracy: 0.7778 - val_loss: 0.2236\n",
      "Epoch 70/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9390 - loss: 0.2818 - val_accuracy: 1.0000 - val_loss: 0.0344\n",
      "Epoch 71/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9612 - loss: 0.1932 - val_accuracy: 0.7778 - val_loss: 0.4335\n",
      "Epoch 72/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.9220 - loss: 0.2101 - val_accuracy: 1.0000 - val_loss: 0.0865\n",
      "Epoch 73/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9299 - loss: 0.1635 - val_accuracy: 0.8889 - val_loss: 0.1541\n",
      "Epoch 74/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9590 - loss: 0.1624 - val_accuracy: 0.8889 - val_loss: 0.2929\n",
      "Epoch 75/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9782 - loss: 0.1098 - val_accuracy: 1.0000 - val_loss: 0.0830\n",
      "Epoch 76/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.9812 - loss: 0.1021 - val_accuracy: 0.8889 - val_loss: 0.3187\n",
      "Epoch 77/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9580 - loss: 0.1357 - val_accuracy: 0.8889 - val_loss: 0.1603\n",
      "Epoch 78/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9575 - loss: 0.1610 - val_accuracy: 1.0000 - val_loss: 0.0321\n",
      "Epoch 79/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9497 - loss: 0.1474 - val_accuracy: 0.7778 - val_loss: 0.6805\n",
      "Epoch 80/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.9380 - loss: 0.1729 - val_accuracy: 1.0000 - val_loss: 0.0260\n",
      "Epoch 81/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9688 - loss: 0.1258 - val_accuracy: 1.0000 - val_loss: 0.1185\n",
      "Epoch 82/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9577 - loss: 0.1044 - val_accuracy: 1.0000 - val_loss: 0.0645\n",
      "Epoch 83/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.9677 - loss: 0.1014 - val_accuracy: 0.7778 - val_loss: 0.4282\n",
      "Epoch 84/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.9494 - loss: 0.1368 - val_accuracy: 1.0000 - val_loss: 0.0602\n",
      "Epoch 85/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.9632 - loss: 0.1028 - val_accuracy: 0.8889 - val_loss: 0.5055\n",
      "Epoch 86/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9819 - loss: 0.1148 - val_accuracy: 0.8889 - val_loss: 0.1446\n",
      "Epoch 87/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9668 - loss: 0.0874 - val_accuracy: 0.7778 - val_loss: 0.6336\n",
      "Epoch 88/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9778 - loss: 0.1056 - val_accuracy: 1.0000 - val_loss: 0.0542\n",
      "Epoch 89/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9533 - loss: 0.1283 - val_accuracy: 0.7778 - val_loss: 0.9638\n",
      "Epoch 90/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9644 - loss: 0.1417 - val_accuracy: 1.0000 - val_loss: 0.0440\n",
      "Epoch 91/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9434 - loss: 0.1648 - val_accuracy: 0.7778 - val_loss: 0.6639\n",
      "Epoch 92/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9796 - loss: 0.0962 - val_accuracy: 1.0000 - val_loss: 0.0112\n",
      "Epoch 93/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9693 - loss: 0.0874 - val_accuracy: 0.8889 - val_loss: 0.1917\n",
      "Epoch 94/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9479 - loss: 0.1426 - val_accuracy: 0.7778 - val_loss: 0.2034\n",
      "Epoch 95/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.9746 - loss: 0.0980 - val_accuracy: 0.7778 - val_loss: 0.3630\n",
      "Epoch 96/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9560 - loss: 0.1458 - val_accuracy: 0.7778 - val_loss: 1.0092\n",
      "Epoch 97/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9039 - loss: 0.3594 - val_accuracy: 0.7778 - val_loss: 1.4820\n",
      "Epoch 98/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9032 - loss: 0.2768 - val_accuracy: 1.0000 - val_loss: 0.0176\n",
      "Epoch 99/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.9310 - loss: 0.1951 - val_accuracy: 1.0000 - val_loss: 0.1441\n",
      "Epoch 100/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9523 - loss: 0.1538 - val_accuracy: 0.8889 - val_loss: 0.2566\n",
      "Epoch 101/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.9411 - loss: 0.2009 - val_accuracy: 0.7778 - val_loss: 0.4183\n",
      "Epoch 102/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9285 - loss: 0.1979 - val_accuracy: 0.8889 - val_loss: 0.3341\n",
      "Epoch 103/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9781 - loss: 0.1137 - val_accuracy: 0.8889 - val_loss: 0.2427\n",
      "Epoch 104/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9421 - loss: 0.2168 - val_accuracy: 1.0000 - val_loss: 0.0501\n",
      "Epoch 105/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9504 - loss: 0.1316 - val_accuracy: 1.0000 - val_loss: 0.0644\n",
      "Epoch 106/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.9729 - loss: 0.0761 - val_accuracy: 1.0000 - val_loss: 0.1282\n",
      "Epoch 107/2000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9815 - loss: 0.0673 - val_accuracy: 1.0000 - val_loss: 0.1093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14ba5f65690>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, validation_data=(X_test, y_test), callbacks=[tb_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape             </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,112</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ batch_normalization               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                          │               │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                          │               │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└───────────────────────────────────┴──────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │       \u001b[38;5;34m442,112\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ batch_normalization               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                          │               │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                          │               │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                │            \u001b[38;5;34m99\u001b[0m │\n",
       "└───────────────────────────────────┴──────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,790,795</span> (6.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,790,795\u001b[0m (6.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,867</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m596,867\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,193,736</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,193,736\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0112\n",
      "Test Loss: 0.011220559477806091\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891ms/step\n"
     ]
    }
   ],
   "source": [
    "# 2. Prediction on test set\n",
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted class labels (if classification)\n",
    "y_pred_classes = np.argmax(res, axis=1)\n",
    "\n",
    "# Print true class labels\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1 2 2 2 1 1 2 0 0]\n",
      "True labels: [1 2 2 2 1 1 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted labels:\", y_pred_classes)\n",
    "print(\"True labels:\", y_true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add class labels (actions)\n",
    "tick_marks = np.arange(len(actions))\n",
    "plt.xticks(tick_marks, actions, rotation=45)\n",
    "plt.yticks(tick_marks, actions)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "\n",
    "# Annotate each cell in the matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(len(actions)):\n",
    "    for j in range(len(actions)):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "model.save('sign_language_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "model.save('sign_language_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[6, 0],\n",
       "        [0, 3]],\n",
       "\n",
       "       [[5, 0],\n",
       "        [0, 4]]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    # Find the index of the highest probability\n",
    "    max_prob_index = np.argmax(res)\n",
    "    max_prob_value = res[max_prob_index]\n",
    "    sign = actions[max_prob_index]\n",
    "    \n",
    "    # Convert probability to 2 decimal places\n",
    "    prob_value = f\"{max_prob_value:.2f}\"\n",
    "    \n",
    "    # Return the sign and its probability value\n",
    "    return sign, prob_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the path to the video file (without extension):  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predicted sign: hello, Probability: 1.00\n",
      "Video contains 36 frames.\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "# Prompt the user for the video file path\n",
    "video_file = input(\"Please enter the path to the video file (without extension): \")\n",
    "input_folder = \"test_input_data\"\n",
    "\n",
    "# Create the full video file path by appending the extension\n",
    "video_path = os.path.join(input_folder, f\"{video_file}.mp4\")\n",
    "\n",
    "# Check if the video file exists\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"Error: Video file '{video_filename}.mp4' not found in the '{input_folder}' folder.\")\n",
    "    exit()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Failed to open video file at {video_path}\")\n",
    "    exit()\n",
    "    \n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Check if video has frames to read\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"Video contains {frame_count} frames.\")\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Extract keypoints and append to sequence\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]  # Keep only the latest 30 frames\n",
    "\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "\n",
    "            # Display the predicted action\n",
    "            predicted_action, prob_value = prob_viz(res, actions, frame, colors)\n",
    "            \n",
    "            # Output the result to the console\n",
    "            print(f\"Predicted sign: {predicted_action}, Probability: {prob_value}\")\n",
    "            \n",
    "            # Return predicted sign and probability (for function)\n",
    "            # return predicted_sign, prob_value\n",
    "  \n",
    "        # Break with q\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Convert Model to .tflite file for android use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 21 variables whereas the saved optimizer has 40 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('sign_language_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\tanad\\AppData\\Local\\Temp\\tmpta2mdlvj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\tanad\\AppData\\Local\\Temp\\tmpta2mdlvj\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\tanad\\AppData\\Local\\Temp\\tmpta2mdlvj'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 1662), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1424725710736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725706896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725705936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424704751376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424704751952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424704752336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424704750032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725710544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725712464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725707472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725706320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725712848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725705744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725706704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725714192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725713232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725714768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725717456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725709008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725718800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725719184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725713424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1424725718992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_new_converter=True\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                          tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Saving the model.\n",
    "mpath ='sign_language_model.tflite'\n",
    "with open(mpath, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/amir-abdi/keras_to_tensorflow/blob/master/keras_to_tensorflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-input_fld INPUT_FLD] [-output_fld OUTPUT_FLD]\n",
      "                             [-input_model_file INPUT_MODEL_FILE]\n",
      "                             [-output_model_file OUTPUT_MODEL_FILE]\n",
      "                             [-output_graphdef_file OUTPUT_GRAPHDEF_FILE]\n",
      "                             [-num_outputs NUM_OUTPUTS] [-graph_def GRAPH_DEF]\n",
      "                             [-output_node_prefix OUTPUT_NODE_PREFIX]\n",
      "                             [-quantize QUANTIZE]\n",
      "                             [-theano_backend THEANO_BACKEND] [-f F]\n",
      "\n",
      "set input arguments\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -input_fld INPUT_FLD\n",
      "  -output_fld OUTPUT_FLD\n",
      "  -input_model_file INPUT_MODEL_FILE\n",
      "  -output_model_file OUTPUT_MODEL_FILE\n",
      "  -output_graphdef_file OUTPUT_GRAPHDEF_FILE\n",
      "  -num_outputs NUM_OUTPUTS\n",
      "  -graph_def GRAPH_DEF\n",
      "  -output_node_prefix OUTPUT_NODE_PREFIX\n",
      "  -quantize QUANTIZE\n",
      "  -theano_backend THEANO_BACKEND\n",
      "  -f F\n",
      "input args:  Namespace(input_fld='.', output_fld='', input_model_file='model.h5', output_model_file='', output_graphdef_file='model.ascii', num_outputs=1, graph_def=False, output_node_prefix='output_node', quantize=False, theano_backend=False, f='C:\\\\Users\\\\tanad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-48122dba-50bf-40fc-a8e3-05aa66e1324e.json')\n",
      "usage: ipykernel_launcher.py [-h] [-input_fld INPUT_FLD] [-output_fld OUTPUT_FLD]\n",
      "                             [-input_model_file INPUT_MODEL_FILE]\n",
      "                             [-output_model_file OUTPUT_MODEL_FILE]\n",
      "                             [-output_graphdef_file OUTPUT_GRAPHDEF_FILE]\n",
      "                             [-num_outputs NUM_OUTPUTS] [-graph_def GRAPH_DEF]\n",
      "                             [-output_node_prefix OUTPUT_NODE_PREFIX]\n",
      "                             [-quantize QUANTIZE]\n",
      "                             [-theano_backend THEANO_BACKEND] [-f F]\n",
      "\n",
      "set input arguments\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -input_fld INPUT_FLD\n",
      "  -output_fld OUTPUT_FLD\n",
      "  -input_model_file INPUT_MODEL_FILE\n",
      "  -output_model_file OUTPUT_MODEL_FILE\n",
      "  -output_graphdef_file OUTPUT_GRAPHDEF_FILE\n",
      "  -num_outputs NUM_OUTPUTS\n",
      "  -graph_def GRAPH_DEF\n",
      "  -output_node_prefix OUTPUT_NODE_PREFIX\n",
      "  -quantize QUANTIZE\n",
      "  -theano_backend THEANO_BACKEND\n",
      "  -f F\n",
      "input args:  Namespace(input_fld='.', output_fld='', input_model_file='sign_language_model.keras', output_model_file='', output_graphdef_file='model.ascii', num_outputs=1, graph_def=False, output_node_prefix='output_node', quantize=False, theano_backend=False, f='C:\\\\Users\\\\tanad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-48122dba-50bf-40fc-a8e3-05aa66e1324e.json')\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='set input arguments')\n",
    "parser.add_argument('-input_fld', action=\"store\", \n",
    "                    dest='input_fld', type=str, default='.')\n",
    "parser.add_argument('-output_fld', action=\"store\", \n",
    "                    dest='output_fld', type=str, default='')\n",
    "parser.add_argument('-input_model_file', action=\"store\", \n",
    "                    dest='input_model_file', type=str, default='model.h5')\n",
    "parser.add_argument('-output_model_file', action=\"store\", \n",
    "                    dest='output_model_file', type=str, default='')\n",
    "parser.add_argument('-output_graphdef_file', action=\"store\", \n",
    "                    dest='output_graphdef_file', type=str, default='model.ascii')\n",
    "parser.add_argument('-num_outputs', action=\"store\", \n",
    "                    dest='num_outputs', type=int, default=1)\n",
    "parser.add_argument('-graph_def', action=\"store\", \n",
    "                    dest='graph_def', type=bool, default=False)\n",
    "parser.add_argument('-output_node_prefix', action=\"store\", \n",
    "                    dest='output_node_prefix', type=str, default='output_node')\n",
    "parser.add_argument('-quantize', action=\"store\", \n",
    "                    dest='quantize', type=bool, default=False)\n",
    "parser.add_argument('-theano_backend', action=\"store\", \n",
    "                    dest='theano_backend', type=bool, default=False)\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "parser.print_help()\n",
    "print('input args: ', args)\n",
    "\n",
    "if args.theano_backend is True and args.quantize is True:\n",
    "    raise ValueError(\"Quantize feature does not work with theano backend.\")\n",
    "parser = argparse.ArgumentParser(description='set input arguments')\n",
    "parser.add_argument('-input_fld', action=\"store\", \n",
    "                    dest='input_fld', type=str, default='.')\n",
    "parser.add_argument('-output_fld', action=\"store\", \n",
    "                    dest='output_fld', type=str, default='')\n",
    "parser.add_argument('-input_model_file', action=\"store\", \n",
    "                    dest='input_model_file', type=str, default='sign_language_model.keras')\n",
    "parser.add_argument('-output_model_file', action=\"store\", \n",
    "                    dest='output_model_file', type=str, default='')\n",
    "parser.add_argument('-output_graphdef_file', action=\"store\", \n",
    "                    dest='output_graphdef_file', type=str, default='model.ascii')\n",
    "parser.add_argument('-num_outputs', action=\"store\", \n",
    "                    dest='num_outputs', type=int, default=1)\n",
    "parser.add_argument('-graph_def', action=\"store\", \n",
    "                    dest='graph_def', type=bool, default=False)\n",
    "parser.add_argument('-output_node_prefix', action=\"store\", \n",
    "                    dest='output_node_prefix', type=str, default='output_node')\n",
    "parser.add_argument('-quantize', action=\"store\", \n",
    "                    dest='quantize', type=bool, default=False)\n",
    "parser.add_argument('-theano_backend', action=\"store\", \n",
    "                    dest='theano_backend', type=bool, default=False)\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "parser.print_help()\n",
    "print('input args: ', args)\n",
    "\n",
    "if args.theano_backend is True and args.quantize is True:\n",
    "    raise ValueError(\"Quantize feature does not work with theano backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import backend as K\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "output_fld =  args.input_fld if args.output_fld == '' else args.output_fld\n",
    "if args.output_model_file == '':\n",
    "    args.output_model_file = str(Path(args.input_model_file).name) + '.pb'\n",
    "Path(output_fld).mkdir(parents=True, exist_ok=True)    \n",
    "weight_file_path = str(Path(args.input_fld) / args.input_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 21 variables whereas the saved optimizer has 40 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_output):\n\u001b[0;32m     23\u001b[0m     pred_node_names[i] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39moutput_node_prefix\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\n\u001b[1;32m---> 24\u001b[0m     pred[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_node_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput nodes names are: \u001b[39m\u001b[38;5;124m'\u001b[39m, pred_node_names)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "\n",
    "if args.theano_backend:\n",
    "    K.set_image_data_format('channels_first')\n",
    "else:\n",
    "    K.set_image_data_format('channels_last')\n",
    "\n",
    "try:\n",
    "    net_model = load_model(weight_file_path)\n",
    "except ValueError as err:\n",
    "    print('''Input file specified ({}) only holds the weights, and not the model defenition.\n",
    "    Save the model using mode.save(filename.h5) which will contain the network architecture\n",
    "    as well as its weights. \n",
    "    If the model is saved using model.save_weights(filename.h5), the model architecture is \n",
    "    expected to be saved separately in a json format and loaded prior to loading the weights.\n",
    "    Check the keras documentation for more details (https://keras.io/getting-started/faq/)'''\n",
    "          .format(weight_file_path))\n",
    "    raise err\n",
    "num_output = args.num_outputs\n",
    "pred = [None]*num_output\n",
    "pred_node_names = [None]*num_output\n",
    "for i in range(num_output):\n",
    "    pred_node_names[i] = args.output_node_prefix+str(i)\n",
    "    pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\n",
    "print('output nodes names are: ', pred_node_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert variables to constants and save\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "if args.quantize:\n",
    "    transforms = [\"quantize_weights\", \"quantize_nodes\"]\n",
    "    transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [], pred_node_names, transforms)\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, transformed_graph_def, pred_node_names)\n",
    "else:\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)    \n",
    "graph_io.write_graph(constant_graph, output_fld, args.output_model_file, as_text=False)\n",
    "print('saved the freezed graph (ready for inference) at: ', str(Path(output_fld) / args.output_model_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
